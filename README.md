# HB-Eval: Procedural Fairness Evaluation for Agentic AI

<p align="left">
  <a href="https://opensource.org/licenses/Apache-2.0"><img src="https://img.shields.io/badge/License-Apache%202.0-blue.svg" alt="License"></a>
  <a href="https://www.python.org/"><img src="https://img.shields.io/badge/Python-3.8%2B-yellow.svg" alt="Python"></a>
  <img src="https://img.shields.io/badge/Status-Anonymous%20Review-orange" alt="Status">
  <img src="https://img.shields.io/badge/ICLR%202026-AFAA%20Workshop-blue" alt="ICLR">
  <img src="https://img.shields.io/badge/Mobile-Ready-green" alt="Mobile">
</p>

---

## ğŸ¯ **Overview**

**Anonymous submission for ICLR 2026 Workshop on Algorithmic Fairness across Alignment procedures and Agentic systems (AFAA).**

HB-Eval establishes that **reliability is a fundamental prerequisite for algorithmic fairness** in autonomous agents. An agent with high behavioral variance or unpredictable failures under identical constraints is inherently *procedurally unfair*.

### **The Problem**
```
Agent A: 70% success, Ïƒ=0.08  â†’ Consistent (procedurally fair)
Agent B: 70% success, Ïƒ=0.14  â†’ Volatile (procedurally unfair)

Same average, drastically different fairness.
```

### **Our Solution**
Episode-level evaluation (K=5 repeated attempts) reveals:
- **Variance**: 1.75Ã— difference across agents â†’ unequal treatment
- **Systematic Bias**: 68% failures in 23% of categories â†’ targeted unfairness  
- **Perturbation Brittleness**: 34% degradation â†’ arbitrary sensitivity

**Integrated system achieves 37% variance reduction and 18% robustness improvement.**

---

## ğŸš€ **Quick Start**

### **Option 1: Full Framework (Recommended)**
```bash
git clone https://github.com/[anonymous]/hb-eval-framework.git
cd hb-eval-framework
pip install -r requirements.txt

# Run evaluation
python examples/basic_evaluation.py
```

### **Option 2: Zero-Dependency Mobile Demo** ğŸ“±
```bash
# Works on phones (Pydroid3), embedded systems, anywhere!
cd mobile_demos
python demo_stable.py        # Scenario 1: Stable agent
python demo_high_risk.py     # Scenario 2: High-risk detection
python demo_adaptive.py      # Scenario 3: Adaptive mitigation
```

**No numpy, no pandas, no ML libraries required.** Pure Python for maximum accessibility.

---

## ğŸ“Š **Key Results**

| Metric | Finding | Implication |
|--------|---------|-------------|
| **Behavioral Variance** | 8-14% (1.75Ã— difference) | Unequal treatment of equivalent scenarios |
| **Systematic Bias** | 68% failures in 23% categories | Concentrated unfairness amenable to mitigation |
| **Perturbation Sensitivity** | 12-34% degradation | Arbitrary surface-form dependencies |
| **System Impact** | 37% variance â†“, 18% robustness â†‘ | Quantifiable procedural fairness improvement |

---

## ğŸ—ï¸ **Architecture**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HB-Eval: Detection (variance, clustering, robustness)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“ Violations detected
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Adapt-Plan: Correction (PEI monitoring, replanning) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“ References retrieved
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EDM: Certification (episode storage, FRR, TI=0.98)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“ Evidence extracted
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HCI-EDM: Accountability (explanations, audit UI)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“± **Mobile Demos - Zero Dependencies**

Three progressive scenarios demonstrating core concepts:

### **Demo 1: Stable Agent Detection**
```python
python mobile_demos/demo_stable.py
```
**Output:**
```
Variance: 0.0162
Status: Stable âœ“
Decision: No intervention needed
```
â†’ System correctly identifies procedurally fair behavior

### **Demo 2: High-Risk Detection**
```python
python mobile_demos/demo_high_risk.py
```
**Output:**
```
Success Rate: 0.35
Variance: 0.2275
Status: High risk detected âš ï¸
Certified Memory: None
```
â†’ System detects procedural unfairness but lacks correction data

### **Demo 3: Adaptive Mitigation**
```python
python mobile_demos/demo_adaptive.py
```
**Output:**
```
Variance: 0.2475
Decision: RISK_MITIGATION
Action: Prevent unfair deployment âœ“
```
â†’ System enters safe mode, blocking procedurally unfair agent

**These demos validate framework logic on any deviceâ€”phones, IoT, embedded systemsâ€”proving implementation-agnostic reliability assessment.**

---

## ğŸ”¬ **Core Metrics**

| Metric | Formula | Fairness Dimension |
|--------|---------|-------------------|
| **Variance (ÏƒÂ²)** | `Var(successes)` | Procedural consistency |
| **Behavioral Consistency (BC)** | `1 - EditDist(sequences)` | Decision stability |
| **Perturbation Robustness (PR)** | `1 - mean(degradation)` | Semantic fairness |
| **Failure Clustering (FC)** | Entropy of error distribution | Systematic bias |
| **Planning Efficiency (PEI)** | `GoalAchievement / Cost` | Resource fairness |
| **Failure Resilience (FRR)** | `Recoveries / Failures` | Learning capacity |
| **Traceability Index (TI)** | `Logged / Total` | Audit compliance |

---

## ğŸ“‚ **Repository Structure**

```
hb-eval-framework/
â”œâ”€â”€ hb_eval/              # Core evaluation framework
â”‚   â”œâ”€â”€ evaluator.py      # Episode-level evaluation
â”‚   â”œâ”€â”€ metrics.py        # Fairness metrics
â”‚   â””â”€â”€ perturbations.py  # Semantic-preserving variations
â”‚
â”œâ”€â”€ mobile_demos/         # ğŸ”¥ Zero-dependency demos
â”‚   â”œâ”€â”€ demo_stable.py
â”‚   â”œâ”€â”€ demo_high_risk.py
â”‚   â””â”€â”€ demo_adaptive.py
â”‚
â”œâ”€â”€ examples/             # Integration examples
â”‚   â”œâ”€â”€ basic_evaluation.py
â”‚   â””â”€â”€ integrated_system.py
â”‚
â””â”€â”€ docs/                 # Documentation
    â”œâ”€â”€ getting_started.md
    â””â”€â”€ metrics_guide.md
```

---

## ğŸ’¡ **Why This Matters**

### **For Research**
- First framework linking reliability to procedural fairness
- Episode-level methodology reveals hidden unfairness
- Reproducible across environments (desktop â†’ mobile)

### **For Deployment**
- Pre-deployment fairness assessment
- Real-time monitoring and correction
- Audit-ready compliance (TI=0.98)

### **For Education**
- Zero-dependency demos for teaching
- Progressive complexity (3 scenarios)
- Accessible on any device

---

## ğŸ§ª **Reproducing Paper Results**

```bash
# Table 1: Main results (WebArena + ALFWorld)
python examples/reproduce_table1.py

# Figure 1: Reliability gap visualization
python examples/reproduce_figure1.py

# End-to-end integrated system demo
python examples/integrated_system.py
```

---

## ğŸ¯ **Use Cases**

### **1. Fairness Assessment**
```python
from hb_eval import EpisodeLevelEvaluator

evaluator = EpisodeLevelEvaluator(K=5)
profile = evaluator.evaluate(agent, tasks)

if profile.variance > 0.12:
    print("âš ï¸ Procedural unfairness detected")
```

### **2. Real-Time Monitoring**
```python
from adapt_plan import AdaptivePlanner

planner = AdaptivePlanner(pei_threshold=0.5)
if planner.compute_pei(trajectory) < 0.5:
    corrected_plan = planner.replan(memory.retrieve())
```

### **3. Audit Compliance**
```python
from edm import EpisodicMemory

memory = EpisodicMemory()
memory.store(episode)
assert memory.traceability_index >= 0.98  # Regulatory requirement
```

---

## ğŸ“– **Documentation**

- **[Getting Started](docs/getting_started.md)** - Installation and first steps
- **[Metrics Guide](docs/metrics_guide.md)** - Detailed metric explanations
- **[Mobile Demos](mobile_demos/README.md)** - Zero-dependency scenarios

---

## ğŸ¤ **Contributing**

This repository is under anonymous review for ICLR 2026 AFAA Workshop.

**During review period:** Please direct questions through conference submission system.

**After acceptance:** Full contribution guidelines will be published.

---

## ğŸ“„ **Citation**

```bibtex
@inproceedings{anonymous2026hbeval,
  title={HB-Eval: Episode-Level Reliability Evaluation for Agentic AI Systems},
  author={Anonymous Authors},
  booktitle={ICLR 2026 Workshop on AFAA},
  year={2026},
  note={Under review}
}
```

---

## ğŸ›¡ï¸ **License**

Apache License 2.0

---

## â­ **Highlights**

- âœ… **Mobile-Ready**: Runs on phones with zero dependencies
- âœ… **Fairness-First**: Reliability operationalized as procedural fairness
- âœ… **Integrated System**: Detection â†’ Correction â†’ Certification â†’ Accountability
- âœ… **Reproducible**: Identical results across platforms
- âœ… **Accessible**: Progressive demos from simple to advanced

---

<p align="center">
  <b>Anonymous Submission for ICLR 2026 AFAA Workshop</b><br>
  <i>"Reliability is not orthogonal to fairnessâ€”it is constitutive of it."</i>
</p>

---

**Last Updated:** January 2026  
**Status:** Under Anonymous Review# HB-Eval: Procedural Fairness Evaluation for Agentic AI Systems

<p align="left">
  <a href="https://opensource.org/licenses/Apache-2.0"><img src="https://img.shields.io/badge/License-Apache%202.0-blue.svg" alt="License"></a>
  <a href="https://www.python.org/"><img src="https://img.shields.io/badge/Python-3.8%2B-yellow.svg" alt="Python"></a>
  <img src="https://img.shields.io/badge/Status-Anonymous%20Review-orange" alt="Status">
  <img src="https://img.shields.io/badge/ICLR%202026-AFAA%20Workshop-blue" alt="ICLR">
</p>

---

## ğŸ¯ **Project Overview**

This repository contains the **anonymous implementation** of the **HB-Eval framework** and its integrated components, submitted to the **ICLR 2026 Workshop on Algorithmic Fairness across Alignment procedures and Agentic systems (AFAA)**.

**Framework Components:**
1. **HB-Eval** - Episode-level behavioral certification for procedural fairness assessment
2. **Adapt-Plan** - Real-time efficiency-aware planning with alignment correction
3. **EDM (Evaluation-Driven Memory)** - Certified episodic memory with audit trails
4. **HCI-EDM** - Performance-grounded interpretability for human accountability

**Core Contribution:**  
We establish that **reliability is a fundamental prerequisite for algorithmic fairness** in autonomous agentic systems. An agent exhibiting high behavioral variance or unpredictable failures under identical constraints is inherently *procedurally unfair*, as it treats equivalent scenarios inconsistently.

> **âš ï¸ Anonymous Submission Notice**  
> This repository is maintained for double-blind peer review. All identifying information has been removed.  
> **Full release with complete documentation will follow upon acceptance.**

---

## ğŸ”¬ **What is HB-Eval?**

Current agent benchmarks test systems once per task and report aggregate success rates. But **averages hide unfairness**.

Consider two AI agents, both succeeding 70% of the time:
- **Agent A**: Low variance (Ïƒ=0.08) â†’ consistent experience for all users
- **Agent B**: High variance (Ïƒ=0.14) â†’ some users get instant solutions, others suffer through failures

**Same average, drastically different procedural fairness.**

HB-Eval reveals this hidden unfairness by:
- Testing agents across **K=5 repeated episodes** with controlled variations
- Applying **realistic perturbations** (paraphrasing, formatting, context changes)
- Measuring **variance, robustness, clustering, and consistency**
- Detecting **systematic bias** where failures concentrate in specific task categories

---

## ğŸ“Š **Key Findings**

Evaluation of three architectures (ReAct, Reflexion, Tree-of-Thought) on WebArena and ALFWorld reveals:

| Metric | Finding | Fairness Implication |
|--------|---------|---------------------|
| **Behavioral Variance** | Differs **1.75Ã—** (Ïƒ: 0.08â€“0.14) | Unequal treatment of equivalent scenarios |
| **Systematic Bias** | **68%** of failures in **23%** of categories | Non-uniform risk distribution |
| **Perturbation Sensitivity** | Up to **34%** degradation | Arbitrary surface-form dependencies |
| **Behavioral Instability** | **3.2Ã—** higher divergence in failures | Unpredictable decision processes |

**With Integrated System:**
- **37%** variance reduction (p<0.001)
- **18%** perturbation robustness improvement (p<0.001)  
- **23%** planning efficiency increase via Adapt-Plan
- **98%** traceability for audit compliance (TI=0.98)

---

## ğŸ—ï¸ **System Architecture**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HB-Eval Framework                         â”‚
â”‚  Episode Expansion â€¢ Variance Analysis â€¢ Failure Clustering  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ Detected Violations
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Adapt-Plan                               â”‚
â”‚    PEI Monitoring â€¢ Real-time Correction â€¢ Efficiency        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ Certified References
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         EDM (Evaluation-Driven Memory)                       â”‚
â”‚   Episode Storage â€¢ FRR Tracking â€¢ Audit Trail (TI=0.98)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ Evidence Extraction
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HCI-EDM                                   â”‚
â”‚  Performance-Grounded Explanations â€¢ Fairness Audit UI       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Data Flow:**  
Detection â†’ Correction â†’ Certification â†’ Accountability

---

## ğŸš€ **Quick Start**

### **Installation**

```bash
git clone https://github.com/[anonymous-repo]/hb-eval-framework.git
cd hb-eval-framework
pip install -r requirements.txt
```

### **Basic Usage**

```python
from hb_eval import EpisodeLevelEvaluator
from adapt_plan import AdaptivePlanner
from edm import EpisodicMemory
from hci_edm import ExplanationGenerator

# Initialize integrated system
evaluator = EpisodeLevelEvaluator(K=5, perturbations=['paraphrase', 'context', 'format'])
planner = AdaptivePlanner(pei_threshold=0.5)
memory = EpisodicMemory()
explainer = ExplanationGenerator(memory)

# Evaluate agent with procedural fairness monitoring
reliability_profile = evaluator.evaluate(
    agent=your_agent,
    tasks=webarena_tasks,
    memory=memory,
    planner=planner
)

# Generate fairness audit report
fairness_report = explainer.generate_report(reliability_profile)
print(f"Variance: {reliability_profile.variance:.3f}")
print(f"Perturbation Robustness: {reliability_profile.robustness:.3f}")
print(f"Systematic Bias Detected: {reliability_profile.failure_clustering}")
```

### **Run Full Evaluation**

```bash
# WebArena evaluation (100 tasks, K=5 episodes each)
python scripts/evaluate_webarena.py --agent ReAct --K 5

# ALFWorld evaluation (50 tasks, K=5 episodes each)
python scripts/evaluate_alfworld.py --agent Reflexion --K 5

# End-to-end integrated system demo
python scripts/integrated_demo.py
```

---

## ğŸ“‚ **Repository Structure**

```
hb-eval-framework/
â”œâ”€â”€ hb_eval/                    # Core evaluation framework
â”‚   â”œâ”€â”€ evaluator.py            # Episode-level evaluation
â”‚   â”œâ”€â”€ perturbations.py        # Paraphrase/context/format variations
â”‚   â”œâ”€â”€ metrics.py              # Variance, robustness, consistency
â”‚   â””â”€â”€ clustering.py           # Failure mode analysis
â”‚
â”œâ”€â”€ adapt_plan/                 # Real-time alignment correction
â”‚   â”œâ”€â”€ planner.py              # Adaptive planning with PEI monitoring
â”‚   â”œâ”€â”€ efficiency.py           # Planning Efficiency Index (PEI)
â”‚   â””â”€â”€ correction.py           # Replanning triggers
â”‚
â”œâ”€â”€ edm/                        # Certified episodic memory
â”‚   â”œâ”€â”€ memory.py               # Episode storage with indexing
â”‚   â”œâ”€â”€ certification.py        # FRR and TI computation
â”‚   â””â”€â”€ retrieval.py            # Similarity-based episode retrieval
â”‚
â”œâ”€â”€ hci_edm/                    # Human-interpretable explanations
â”‚   â”œâ”€â”€ explanations.py         # Performance-grounded interpretation
â”‚   â”œâ”€â”€ visualization.py        # Fairness audit dashboards
â”‚   â””â”€â”€ reporting.py            # Audit-ready reports
â”‚
â”œâ”€â”€ scripts/                    # Evaluation scripts
â”‚   â”œâ”€â”€ evaluate_webarena.py
â”‚   â”œâ”€â”€ evaluate_alfworld.py
â”‚   â””â”€â”€ integrated_demo.py
â”‚
â”œâ”€â”€ benchmarks/                 # Benchmark integration
â”‚   â”œâ”€â”€ webarena_adapter.py
â”‚   â””â”€â”€ alfworld_adapter.py
â”‚
â”œâ”€â”€ examples/                   # Usage examples
â”‚   â”œâ”€â”€ basic_evaluation.py
â”‚   â”œâ”€â”€ fairness_audit.py
â”‚   â””â”€â”€ integrated_system.py
â”‚
â”œâ”€â”€ tests/                      # Unit and integration tests
â”œâ”€â”€ docs/                       # Documentation
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

---

## ğŸ“Š **Core Metrics**

### **HB-Eval Metrics**

| Metric | Formula | Fairness Dimension |
|--------|---------|-------------------|
| **Success Variance (ÏƒÂ²)** | `Var(ğŸ™_success)` | Procedural consistency |
| **Behavioral Consistency (BC)** | `1 - mean(EditDist)` | Decision stability |
| **Perturbation Robustness (PR)** | `1 - mean(Î”SR)` | Semantic fairness |
| **Failure Clustering (FC)** | Entropy of error distribution | Systematic bias detection |

### **Adapt-Plan Metrics**

| Metric | Formula | Purpose |
|--------|---------|---------|
| **Planning Efficiency Index (PEI)** | `GoalAchievement / ActionCost Ã— (1 - Redundancy)` | Efficiency monitoring |

### **EDM Metrics**

| Metric | Formula | Purpose |
|--------|---------|---------|
| **Failure Resilience Rate (FRR)** | `#Recoveries / #Failures` | Learning from mistakes |
| **Traceability Index (TI)** | `LoggedActions / TotalActions` | Audit compliance |

---

## ğŸ”¬ **Reproducibility**

All results in the paper are fully reproducible:

```bash
# Reproduce Table 1 (Main results)
python scripts/reproduce_table1.py

# Reproduce Figure 1 (Reliability Gap)
python scripts/reproduce_figure1.py

# Reproduce Figure 2 (Workflow)
python scripts/reproduce_figure2.py

# Reproduce Figure 3 (Certified Reliability)
python scripts/reproduce_figure3.py

# Statistical significance tests
python scripts/run_statistical_tests.py
```

**Computational Requirements:**
- WebArena: ~42 GPU-hours for 100 tasks Ã— 5 episodes
- ALFWorld: ~18 GPU-hours for 50 tasks Ã— 5 episodes
- Total storage: ~10.4 GB for all traces

---

## ğŸ¯ **Use Cases**

### **1. Pre-Deployment Fairness Assessment**
```python
# Assess procedural fairness before deployment
profile = evaluator.evaluate(candidate_agent, production_tasks)
if profile.variance > 0.12:
    print("âš ï¸ High variance detected - procedural unfairness risk")
if profile.failure_clustering['forms'] > 0.5:
    print("âš ï¸ Systematic bias: 50%+ failures on form tasks")
```

### **2. Real-Time Monitoring**
```python
# Monitor PEI during operation and trigger corrections
while agent.running:
    pei = planner.compute_pei(agent.current_trajectory)
    if pei < 0.5:
        corrected_plan = planner.replan(memory.retrieve_successful())
        agent.execute(corrected_plan)
```

### **3. Audit Trail Generation**
```python
# Generate compliance-ready audit reports
audit_report = explainer.generate_audit_report(
    time_range="2025-01-01 to 2025-01-31",
    agent_id="production-agent-v2.3"
)
# TI=0.98 ensures 98% of actions logged for regulatory review
```

### **4. Fairness-Aware Agent Selection**
```python
# Compare agents on fairness dimensions, not just accuracy
agents = [ReAct(), Reflexion(), TreeOfThought()]
profiles = [evaluator.evaluate(agent, tasks) for agent in agents]

# Rank by composite fairness score
fairness_ranking = sorted(profiles, 
    key=lambda p: (p.variance, -p.robustness, -p.consistency))
```

---

## ğŸ“ˆ **Benchmarks**

### **Supported Environments**

| Benchmark | Tasks | Modality | Difficulty |
|-----------|-------|----------|-----------|
| **WebArena** | 812 | Web navigation | High |
| **ALFWorld** | 134 | Embodied household | Medium |
| **SWE-bench** | 2,294 | Code generation | Very High |
| **Custom** | Any | Agent-defined | Variable |

### **Evaluated Architectures**

| Agent | Paradigm | Base Model |
|-------|----------|------------|
| **ReAct** | Single-pass reasoning | GPT-4-turbo |
| **Reflexion** | Iterative self-correction | GPT-4-turbo |
| **Tree-of-Thought** | Search-based planning | GPT-4-turbo |

---

## ğŸ”„ **Extension Guide**

### **Adding New Agents**

```python
from hb_eval import AgentInterface

class MyCustomAgent(AgentInterface):
    def step(self, observation):
        # Your agent logic
        action = self.policy(observation)
        return action
    
    def reset(self):
        # Reset agent state
        pass

# Evaluate with HB-Eval
profile = evaluator.evaluate(MyCustomAgent(), tasks)
```

### **Adding New Perturbations**

```python
from hb_eval.perturbations import PerturbationBase

class MyPerturbation(PerturbationBase):
    def apply(self, task_description):
        # Transform task while preserving semantics
        return perturbed_description

evaluator.add_perturbation('my_perturbation', MyPerturbation())
```

### **Custom Fairness Metrics**

```python
from hb_eval.metrics import MetricBase

class DemographicParity(MetricBase):
    def compute(self, episodes, demographics):
        # Compute fairness metric
        return parity_score

evaluator.add_metric('demographic_parity', DemographicParity())
```

---

## ğŸ§ª **Testing**

```bash
# Run all tests
pytest tests/

# Run specific test suites
pytest tests/test_hb_eval.py       # Core evaluation tests
pytest tests/test_adapt_plan.py    # Planning tests
pytest tests/test_edm.py           # Memory tests
pytest tests/test_hci_edm.py       # Explanation tests

# Run integration tests
pytest tests/integration/

# Generate coverage report
pytest --cov=hb_eval --cov-report=html
```

---

## ğŸ“– **Documentation**

Detailed documentation is available in the `docs/` directory:

- **[Getting Started](docs/getting_started.md)** - Installation and basic usage
- **[API Reference](docs/api_reference.md)** - Complete API documentation
- **[Metrics Guide](docs/metrics.md)** - Detailed metric explanations
- **[Integration Guide](docs/integration.md)** - Integrating with existing systems
- **[Fairness Theory](docs/fairness_theory.md)** - Procedural fairness background
- **[Case Studies](docs/case_studies.md)** - Real-world applications

---

## ğŸ¤ **Contributing**

Contributions are welcome after the review period. For now, please direct questions to the anonymous submission system.

---

## ğŸ“„ **Citation**

```bibtex
@inproceedings{anonymous2026hbeval,
  title={HB-Eval: Episode-Level Reliability Evaluation for Agentic AI Systems},
  author={Anonymous Authors},
  booktitle={ICLR 2026 Workshop on Algorithmic Fairness across Alignment procedures and Agentic systems},
  year={2026},
  note={Under review}
}
```

**Related Work:**
```bibtex
@inproceedings{anonymous2026adaptplan,
  title={Adapt-Plan: Efficiency-Aware Planning with Real-Time Alignment},
  author={Anonymous Authors},
  booktitle={ICLR 2026 Workshop on AFAA},
  year={2026},
  note={Under review}
}

@inproceedings{anonymous2026edm,
  title={EDM: Certified Episodic Memory for Accountable Agentic Systems},
  author={Anonymous Authors},
  booktitle={ICLR 2026 Workshop on AFAA},
  year={2026},
  note={Under review}
}

@inproceedings{anonymous2026hciedm,
  title={HCI-EDM: Performance-Grounded Interpretability for Fair Agents},
  author={Anonymous Authors},
  booktitle={ICLR 2026 Workshop on AFAA},
  year={2026},
  note={Under review}
}
```

---

## ğŸ›¡ï¸ **License**

This project is licensed under the **Apache License 2.0**.

---

## ğŸ’¬ **FAQ**

**Q: Why focus on procedural fairness instead of demographic fairness?**  
A: Procedural fairness (consistent treatment of equivalent scenarios) is a prerequisite for responsible deployment. We focus on behavioral consistency as a measurable dimension of fairness. Demographic fairness integration is planned for future work.

**Q: How does HB-Eval differ from robustness testing?**  
A: Robustness testing typically measures worst-case adversarial degradation. HB-Eval measures typical-case variation under realistic, benign perturbations (how users naturally phrase tasks) to assess procedural consistency.

**Q: Can HB-Eval guarantee formal fairness?**  
A: No. HB-Eval provides empirical characterization of procedural fairness through behavioral measurement. It complements but does not replace formal verification methods.

**Q: What is the computational overhead?**  
A: K=5 episodes increases evaluation cost 3-5Ã— compared to single-episode benchmarks. This is acceptable for pre-deployment assessment and periodic auditing but may be prohibitive for rapid iteration.

**Q: Does this work with LLM-based agents only?**  
A: No. HB-Eval is model-agnostic and evaluates system-level behavior regardless of the underlying architecture (rule-based, classical ML, or LLMs).

**Q: When will the code be fully released?**  
A: Full release with complete documentation, tutorials, and pre-trained models will follow upon paper acceptance.

---

## ğŸ“§ **Contact**

For questions about this anonymous submission, please use the conference submission system.

**After acceptance, contact information will be provided.**

---

<p align="center">
  <b>Anonymous Submission for ICLR 2026 AFAA Workshop</b><br>
  <i>"Reliability is not orthogonal to fairnessâ€”it is constitutive of it."</i>
</p>

---

## ğŸ”— **Related Resources**

- **ICLR 2026 AFAA Workshop**: [Link will be added]
- **Supplementary Materials**: Available in submission system
- **Interactive Demo**: [Link will be added upon acceptance]

---

## ğŸŒŸ **Acknowledgments**

We thank the anonymous reviewers for their valuable feedback and the organizers of the ICLR 2026 AFAA Workshop for creating this important venue for fairness research.

---

**Last Updated:** January 2026  
**Status:** Under Anonymous Review for ICLR 2026 AFAA Workshop
